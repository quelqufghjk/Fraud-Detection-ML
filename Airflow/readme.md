âœ… 1. dag_pipeline_startup.py

Objectif : DÃ©marrer Kafka + Zookeeper, crÃ©er le topic transactions et lancer le producteur

TÃ¢ches :

start_kafka_stack : Lancer Kafka via Docker Compose

create_kafka_topic : CrÃ©er le topic (si non existant)

run_producer_script : Lancer producer.py

FrÃ©quence : Manuelle (au lancement du pipeline)

âœ… 2. dag_scoring.py

Objectif : DÃ©marrer le consumer pour scorer en temps rÃ©el les transactions

TÃ¢ches :

run_consumer_script : ExÃ©cuter consumer.py (Kafka â†’ ML â†’ SQL Server)

FrÃ©quence : Toutes les 5 minutes (*/5 * * * *)

âœ… 3. dag_training.py

Objectif : RÃ©entraÃ®ner le modÃ¨le avec les donnÃ©es les plus rÃ©centes

TÃ¢ches :

run_training_script : Lancer training.py sur augmented_data.parquet

FrÃ©quence : Hebdomadaire ou manuelle (schedule_interval=None)

âœ… 4. dag_backup.py

Objectif : Sauvegarder les prÃ©dictions depuis SQL Server vers un fichier CSV/Parquet

TÃ¢ches :

export_sql_results : Lire la table TransactionsScored

save_to_csv : Ã‰crire dans /outputs/archive/yyyy-mm-dd.csv

FrÃ©quence : Tous les jours Ã  23h (0 23 * * *)

âœ… 5. dag_data_cleaning.py (optionnel)

Objectif : Supprimer les lignes obsolÃ¨tes dans la base SQL Server (vieilles transactions)

TÃ¢ches :

purge_old_records : Supprimer ou archiver les lignes > 30 jours

FrÃ©quence : Mensuelle ou hebdomadaire

âœ… Tips de mise en production

Tous les scripts doivent Ãªtre dans /opt/airflow/dags/scripts/

Utiliser BashOperator pour exÃ©cuter les scripts Python ou shell

Toujours activer les logs et monitoring

Stocker les fichiers exportÃ©s dans un dossier /archive/


### ğŸ”„ Ordre dâ€™exÃ©cution recommandÃ©

1. `dag_pipeline_startup` : DÃ©marre les services et envoie les transactions  
2. `dag_scoring` : Scoring continu des transactions dans Kafka  
3. `dag_backup` : Sauvegarde des prÃ©dictions scorÃ©es (quotidien Ã  23h)
